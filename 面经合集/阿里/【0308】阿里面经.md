阿里大模型面经 0308更新📒
周五的面经，感谢网友分享，提醒码住几道题，拼了一下，不太纯网友说怕被发现透露面经不太好hhh稍微换了一点问法和方式，之后也会这样稍微拼一拼（主要怕被发现，长度外推那里需要学一下，其他的感觉还挺常规的
1. 拷打实习
2. 自注意力机制，详细讲一讲公式？为何除以 根号d_k？了解过kvcache不
3. LLM微调的方法，Lora，adalora了解不
4. RLHF流程?dpo和grpo区别？
5. 为什么sft之后要做强化学习？有对比过做不做的区别么？数据量多少
6. 大模型推理加速，vllm，page attention讲一讲
7. LLM训练中遇到Loss突增如何解决？
8. 了解过长度外推不，如何在一个chat模型上让其可以回答长答案？
9. 力扣：编辑距离