{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入ID形状: torch.Size([1, 6])\n",
      "输出ID形状: torch.Size([1, 4])\n",
      "完整序列形状: torch.Size([1, 10])\n",
      "掩码形状: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "\n",
    "# 设置随机种子以保证可复现性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 定义示例数据 - 整个流程将使用这些数据进行演示\n",
    "input_ids = torch.tensor([[3, 5, 2, 8, 1, 4]])           # [1, 6] - 提示部分\n",
    "output_ids = torch.tensor([[7, 9, 6, 0]])                # [1, 4] - 生成部分\n",
    "full_ids = torch.cat([input_ids, output_ids], dim=1)     # [1, 10] - 完整序列\n",
    "full_mask = torch.ones_like(full_ids)                    # [1, 10] - 序列掩码\n",
    "\n",
    "print(\"输入ID形状:\", input_ids.shape)\n",
    "print(\"输出ID形状:\", output_ids.shape)\n",
    "print(\"完整序列形状:\", full_ids.shape)\n",
    "print(\"掩码形状:\", full_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略模型输出形状: torch.Size([1, 10, 12])\n",
      "参考模型输出形状: torch.Size([1, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "# 创建策略模型和参考模型\n",
    "policy_model = LlamaForCausalLM(config=LlamaConfig(vocab_size=12, num_hidden_layers=1, hidden_size=32))\n",
    "reference_model = deepcopy(policy_model)  # 深度复制确保参数完全相同\n",
    "\n",
    "# 冻结参考模型参数\n",
    "for param in reference_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 对两个模型进行简单测试\n",
    "with torch.no_grad():\n",
    "    policy_outputs = policy_model(full_ids)\n",
    "    ref_outputs = reference_model(full_ids)\n",
    "\n",
    "print(\"策略模型输出形状:\", policy_outputs.logits.shape)  # [batch_size, seq_len, vocab_size]\n",
    "print(\"参考模型输出形状:\", ref_outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奖励模型 - 嵌入输出形状: torch.Size([1, 10, 8])\n",
      "奖励模型 - LSTM输出形状: torch.Size([1, 10, 8])\n",
      "奖励模型 - 最后隐藏状态形状: torch.Size([1, 8])\n",
      "奖励模型 - 输出奖励形状: torch.Size([1])\n",
      "奖励值: 0.29774409532546997\n"
     ]
    }
   ],
   "source": [
    "# 创建奖励模型\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, vocab_size=12, hidden_size=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, masks=None):\n",
    "        # [bs, seq_len] -> [bs, seq_len, hidden]\n",
    "        x = self.embedding(input_ids)\n",
    "        print(f\"奖励模型 - 嵌入输出形状: {x.shape}\")\n",
    "        \n",
    "        outputs, _ = self.lstm(x)\n",
    "        print(f\"奖励模型 - LSTM输出形状: {outputs.shape}\")\n",
    "        \n",
    "        # 只取序列最后一个有效位置的输出\n",
    "        if masks is not None:\n",
    "            last_indices = masks.sum(dim=1) - 1\n",
    "            batch_indices = torch.arange(outputs.size(0))\n",
    "            last_hidden = outputs[batch_indices, last_indices]\n",
    "            print(f\"奖励模型 - 最后隐藏状态形状: {last_hidden.shape}\")\n",
    "        else:\n",
    "            last_hidden = outputs[:, -1]\n",
    "            \n",
    "        # 生成标量奖励\n",
    "        reward = self.head(last_hidden).squeeze(-1)  # [bs]\n",
    "        print(f\"奖励模型 - 输出奖励形状: {reward.shape}\")\n",
    "        \n",
    "        return reward\n",
    "\n",
    "reward_model = RewardModel()\n",
    "# 测试奖励模型\n",
    "reward = reward_model(full_ids, full_mask)\n",
    "print(f\"奖励值: {reward.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值模型 - 嵌入输出形状: torch.Size([1, 10, 8])\n",
      "价值模型 - LSTM输出形状: torch.Size([1, 10, 8])\n",
      "价值模型 - 输出价值形状: torch.Size([1, 10])\n",
      "价值估计:\n",
      "tensor([[-0.2702, -0.2315, -0.2532, -0.2238, -0.2914, -0.2477, -0.2275, -0.3206,\n",
      "         -0.2904, -0.2328]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 创建价值模型\n",
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, vocab_size=12, hidden_size=8):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.head = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        print(f\"价值模型 - 嵌入输出形状: {x.shape}\")\n",
    "        \n",
    "        outputs, _ = self.lstm(x)\n",
    "        print(f\"价值模型 - LSTM输出形状: {outputs.shape}\")\n",
    "        \n",
    "        values = self.head(outputs).squeeze(-1)  # [bs, seq_len]\n",
    "        print(f\"价值模型 - 输出价值形状: {values.shape}\")\n",
    "        \n",
    "        return values\n",
    "\n",
    "critic_model = CriticModel()\n",
    "# 测试价值模型\n",
    "values = critic_model(full_ids)\n",
    "print(\"价值估计:\")\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化前 - 值形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10])\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: -0.13686081767082214\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: -0.13686081767082214\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 1.0405241250991821\n",
      "掩码方差 - 输入形状: torch.Size([1, 10]), 输出: 1.0405241250991821\n",
      "标准化后 - 输出形状: torch.Size([1, 10])\n",
      "原始值: tensor([-0.0111, -0.3385, -0.7628,  0.2919, -0.6887,  2.1098, -0.6621, -1.4626,\n",
      "         1.1491, -0.9935])\n",
      "标准化后: tensor([ 0.1232, -0.1977, -0.6136,  0.4203, -0.5410,  2.2025, -0.5149, -1.2997,\n",
      "         1.2607, -0.8398])\n"
     ]
    }
   ],
   "source": [
    "def masked_mean(values, mask):\n",
    "    \"\"\"计算掩码均值\"\"\"\n",
    "    result = (values * mask).sum() / mask.sum()\n",
    "    print(f\"掩码均值 - 输入形状: {values.shape}, 掩码形状: {mask.shape}, 输出: {result.item()}\")\n",
    "    return result\n",
    "\n",
    "def masked_var(values, mask):\n",
    "    \"\"\"计算掩码方差\"\"\"\n",
    "    mean = masked_mean(values, mask)\n",
    "    result = masked_mean((values - mean) ** 2, mask)\n",
    "    print(f\"掩码方差 - 输入形状: {values.shape}, 输出: {result.item()}\")\n",
    "    return result\n",
    "\n",
    "def masked_whiten(values, mask, shift_mean=True):\n",
    "    \"\"\"对数值进行标准化处理\"\"\"\n",
    "    print(f\"标准化前 - 值形状: {values.shape}, 掩码形状: {mask.shape}\")\n",
    "    mean, var = masked_mean(values, mask), masked_var(values, mask)\n",
    "    whitened = (values - mean) * torch.rsqrt(var + 1e-8) if shift_mean else values * torch.rsqrt(var + 1e-8)\n",
    "    result = whitened * mask\n",
    "    print(f\"标准化后 - 输出形状: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "# 测试掩码操作\n",
    "test_values = torch.randn(1, 10)  # 随机值\n",
    "test_mask = torch.ones(1, 10)  # 全1掩码\n",
    "whitened_values = masked_whiten(test_values, test_mask)\n",
    "\n",
    "print(\"原始值:\", test_values[0])\n",
    "print(\"标准化后:\", whitened_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对数概率计算 - 输入logits形状: torch.Size([1, 10, 12]), 标签形状: torch.Size([1, 10])\n",
      "对数概率分布形状: torch.Size([1, 10, 12])\n",
      "收集对应标签的对数概率形状: torch.Size([1, 10, 1])\n",
      "最终对数概率形状: torch.Size([1, 10])\n",
      "熵计算 - 输入logits形状: torch.Size([1, 10, 12])\n",
      "熵输出形状: torch.Size([1, 10])\n",
      "策略对数概率: tensor([-2.5491, -2.4207, -2.4306, -2.5236, -2.4441, -2.6406, -2.2948, -2.5483,\n",
      "        -2.4693, -2.4820], grad_fn=<SelectBackward0>)\n",
      "策略熵: tensor([2.4801, 2.4831, 2.4791, 2.4823, 2.4804, 2.4769, 2.4790, 2.4770, 2.4779,\n",
      "        2.4822], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def logprobs_from_logits(logits, labels):\n",
    "    \"\"\"计算给定标签的对数概率\"\"\"\n",
    "    print(f\"对数概率计算 - 输入logits形状: {logits.shape}, 标签形状: {labels.shape}\")\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    print(f\"对数概率分布形状: {logp.shape}\")\n",
    "    \n",
    "    logp_labels = torch.gather(logp, dim=-1, index=labels.unsqueeze(-1))\n",
    "    print(f\"收集对应标签的对数概率形状: {logp_labels.shape}\")\n",
    "    \n",
    "    result = logp_labels.squeeze(-1)\n",
    "    print(f\"最终对数概率形状: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    \"\"\"计算策略熵\"\"\"\n",
    "    print(f\"熵计算 - 输入logits形状: {logits.shape}\")\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)\n",
    "    print(f\"熵输出形状: {entropy.shape}\")\n",
    "    return entropy\n",
    "\n",
    "# 测试对数概率和熵计算\n",
    "policy_outputs = policy_model(full_ids)\n",
    "policy_logits = policy_outputs.logits\n",
    "policy_logprobs = logprobs_from_logits(policy_logits, full_ids)\n",
    "entropy = entropy_from_logits(policy_logits)\n",
    "\n",
    "print(\"策略对数概率:\", policy_logprobs[0])\n",
    "print(\"策略熵:\", entropy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对数概率计算 - 输入logits形状: torch.Size([1, 10, 12]), 标签形状: torch.Size([1, 10])\n",
      "对数概率分布形状: torch.Size([1, 10, 12])\n",
      "收集对应标签的对数概率形状: torch.Size([1, 10, 1])\n",
      "最终对数概率形状: torch.Size([1, 10])\n",
      "KL散度计算 - 策略对数概率形状: torch.Size([1, 10]), 参考对数概率形状: torch.Size([1, 10])\n",
      "KL散度输出形状: torch.Size([1, 10])\n",
      "KL散度均值: 0.0\n",
      "KL散度值: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def _kl_penalty(policy_logprobs, ref_logprobs):\n",
    "    \"\"\"计算KL散度惩罚项\"\"\"\n",
    "    print(f\"KL散度计算 - 策略对数概率形状: {policy_logprobs.shape}, 参考对数概率形状: {ref_logprobs.shape}\")\n",
    "    # KL散度: D_KL(P||Q) = E_P[log P - log Q]，这里P是参考模型分布\n",
    "    kl = ref_logprobs - policy_logprobs\n",
    "    print(f\"KL散度输出形状: {kl.shape}\")\n",
    "    print(f\"KL散度均值: {kl.mean().item()}\")\n",
    "    return kl\n",
    "\n",
    "# 计算参考模型的对数概率\n",
    "ref_outputs = reference_model(full_ids)\n",
    "ref_logits = ref_outputs.logits\n",
    "ref_logprobs = logprobs_from_logits(ref_logits, full_ids)\n",
    "\n",
    "# 测试KL散度计算\n",
    "kl = _kl_penalty(policy_logprobs, ref_logprobs)\n",
    "print(\"KL散度值:\", kl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "奖励模型 - 嵌入输出形状: torch.Size([1, 10, 8])\n",
      "奖励模型 - LSTM输出形状: torch.Size([1, 10, 8])\n",
      "奖励模型 - 最后隐藏状态形状: torch.Size([1, 8])\n",
      "奖励模型 - 输出奖励形状: torch.Size([1])\n",
      "奖励计算 - 奖励分数形状: torch.Size([1]), logprobs形状: torch.Size([1, 10])\n",
      "KL散度计算 - 策略对数概率形状: torch.Size([10]), 参考对数概率形状: torch.Size([10])\n",
      "KL散度输出形状: torch.Size([10])\n",
      "KL散度均值: 0.0\n",
      "KL惩罚奖励形状: torch.Size([10])\n",
      "最后一个token位置: 9\n",
      "最终奖励形状: torch.Size([10])\n",
      "批次奖励形状: torch.Size([1, 10])\n",
      "最终奖励值:\n",
      "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        0.2977], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def compute_rewards(scores, logprobs, ref_logprobs, masks, kl_coef=0.1):\n",
    "    \"\"\"计算每个token的奖励，包含KL惩罚\"\"\"\n",
    "    print(f\"奖励计算 - 奖励分数形状: {scores.shape}, logprobs形状: {logprobs.shape}\")\n",
    "    rewards, non_score_rewards, kls = [], [], []\n",
    "    \n",
    "    for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):\n",
    "        # 1. 计算KL散度惩罚\n",
    "        kl = _kl_penalty(logprob, ref_logprob)  # [seq_len]\n",
    "        kls.append(kl)\n",
    "        \n",
    "        # 2. 计算KL惩罚奖励分量 (-kl_coef * kl)\n",
    "        non_score_reward = -kl_coef * kl  # [seq_len]\n",
    "        print(f\"KL惩罚奖励形状: {non_score_reward.shape}\")\n",
    "        non_score_rewards.append(non_score_reward)\n",
    "        \n",
    "        # 3. 初始化总奖励\n",
    "        reward = non_score_reward.clone()  # [seq_len]\n",
    "        \n",
    "        # 4. 找到最后一个非掩码位置索引\n",
    "        last_non_masked_index = mask.nonzero()[-1]\n",
    "        print(f\"最后一个token位置: {last_non_masked_index.item()}\")\n",
    "        \n",
    "        # 5. 将奖励模型评分添加到最后一个有效token\n",
    "        # 这里体现了信用分配：整体奖励由序列末尾向前传播\n",
    "        reward[last_non_masked_index] += score\n",
    "        print(f\"最终奖励形状: {reward.shape}\")\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    stacked_rewards = torch.stack(rewards)\n",
    "    print(f\"批次奖励形状: {stacked_rewards.shape}\")\n",
    "    return stacked_rewards, torch.stack(non_score_rewards), torch.stack(kls)\n",
    "\n",
    "# 测试奖励计算\n",
    "reward_scores = reward_model(full_ids, full_mask)  # [1]\n",
    "rewards, kl_rewards, kls = compute_rewards(reward_scores, policy_logprobs, ref_logprobs, full_mask)\n",
    "\n",
    "print(\"最终奖励值:\")\n",
    "print(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE计算 - 价值形状: torch.Size([1, 10]), 奖励形状: torch.Size([1, 10])\n",
      "\n",
      "== GAE计算详细步骤 ==\n",
      "步骤 t=9:\n",
      "  奖励 r_9 = 0.297744\n",
      "  当前值 V_9 = -0.232775\n",
      "  下一值 V_10 = 0.000000\n",
      "  TD误差 δ_9 = 0.530519\n",
      "  GAE_9 = 0.530519\n",
      "步骤 t=8:\n",
      "  奖励 r_8 = -0.000000\n",
      "  当前值 V_8 = -0.290370\n",
      "  下一值 V_9 = -0.232775\n",
      "  TD误差 δ_8 = 0.059923\n",
      "  GAE_8 = 0.558876\n",
      "步骤 t=7:\n",
      "  奖励 r_7 = -0.000000\n",
      "  当前值 V_7 = -0.320555\n",
      "  下一值 V_8 = -0.290370\n",
      "  TD误差 δ_7 = 0.033088\n",
      "  GAE_7 = 0.558711\n",
      "步骤 t=6:\n",
      "  奖励 r_6 = -0.000000\n",
      "  当前值 V_6 = -0.227476\n",
      "  下一值 V_7 = -0.320555\n",
      "  TD误差 δ_6 = -0.089873\n",
      "  GAE_6 = 0.435595\n",
      "步骤 t=5:\n",
      "  奖励 r_5 = -0.000000\n",
      "  当前值 V_5 = -0.247652\n",
      "  下一值 V_6 = -0.227476\n",
      "  TD误差 δ_5 = 0.022451\n",
      "  GAE_5 = 0.432128\n",
      "步骤 t=4:\n",
      "  奖励 r_4 = -0.000000\n",
      "  当前值 V_4 = -0.291388\n",
      "  下一值 V_5 = -0.247652\n",
      "  TD误差 δ_4 = 0.046212\n",
      "  GAE_4 = 0.452628\n",
      "步骤 t=3:\n",
      "  奖励 r_3 = -0.000000\n",
      "  当前值 V_3 = -0.223812\n",
      "  下一值 V_4 = -0.291388\n",
      "  TD误差 δ_3 = -0.064661\n",
      "  GAE_3 = 0.361035\n",
      "步骤 t=2:\n",
      "  奖励 r_2 = -0.000000\n",
      "  当前值 V_2 = -0.253204\n",
      "  下一值 V_3 = -0.223812\n",
      "  TD误差 δ_2 = 0.031629\n",
      "  GAE_2 = 0.371183\n",
      "步骤 t=1:\n",
      "  奖励 r_1 = -0.000000\n",
      "  当前值 V_1 = -0.231483\n",
      "  下一值 V_2 = -0.253204\n",
      "  TD误差 δ_1 = -0.019189\n",
      "  GAE_1 = 0.329909\n",
      "步骤 t=0:\n",
      "  奖励 r_0 = -0.000000\n",
      "  当前值 V_0 = -0.270201\n",
      "  下一值 V_1 = -0.231483\n",
      "  TD误差 δ_0 = 0.041033\n",
      "  GAE_0 = 0.351312\n",
      "标准化前 - 值形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10])\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 0.43818965554237366\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 0.43818965554237366\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 0.006758241448551416\n",
      "掩码方差 - 输入形状: torch.Size([1, 10]), 输出: 0.006758241448551416\n",
      "标准化后 - 输出形状: torch.Size([1, 10])\n",
      "\n",
      "最终结果 - 优势形状: torch.Size([1, 10]), 回报形状: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "def compute_advantages(values, rewards, mask, gamma=0.99, lam=0.95):\n",
    "    \"\"\"计算广义优势估计(GAE)，修正版本\"\"\"\n",
    "    print(f\"GAE计算 - 价值形状: {values.shape}, 奖励形状: {rewards.shape}\")\n",
    "    \n",
    "    batch_size = values.shape[0]\n",
    "    seq_len = values.shape[1]\n",
    "    advantages = torch.zeros_like(values)  # [batch, seq]\n",
    "    \n",
    "    # 应用掩码\n",
    "    values = values * mask\n",
    "    rewards = rewards * mask\n",
    "    \n",
    "    print(\"\\n== GAE计算详细步骤 ==\")\n",
    "    \n",
    "    # 对每个批次单独计算GAE\n",
    "    for b in range(batch_size):\n",
    "        # 初始化最后一个GAE为0\n",
    "        lastgaelam = 0.0\n",
    "        \n",
    "        # 反向遍历序列\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # 1. 确定下一状态值 \n",
    "            if t == seq_len - 1:\n",
    "                nextvalue = 0.0\n",
    "            else:\n",
    "                nextvalue = values[b, t + 1].item()\n",
    "            \n",
    "            # 2. 计算TD误差: r_t + γV_(t+1) - V_t\n",
    "            current_reward = rewards[b, t].item()\n",
    "            current_value = values[b, t].item()\n",
    "            delta = current_reward + gamma * nextvalue - current_value\n",
    "            \n",
    "            # 3. 更新GAE: δ_t + γλ*GAE_(t+1)\n",
    "            lastgaelam = delta + gamma * lam * lastgaelam\n",
    "            \n",
    "            # 4. 存储到张量中\n",
    "            advantages[b, t] = lastgaelam\n",
    "            \n",
    "            # 5. 打印详情\n",
    "            print(f\"步骤 t={t}:\")\n",
    "            print(f\"  奖励 r_{t} = {current_reward:.6f}\")\n",
    "            print(f\"  当前值 V_{t} = {current_value:.6f}\")\n",
    "            print(f\"  下一值 V_{t+1} = {nextvalue:.6f}\")\n",
    "            print(f\"  TD误差 δ_{t} = {delta:.6f}\")\n",
    "            print(f\"  GAE_{t} = {lastgaelam:.6f}\")\n",
    "    \n",
    "    # 计算回报 = 优势 + 价值\n",
    "    returns = advantages + values\n",
    "    \n",
    "    # 标准化优势\n",
    "    advantages = masked_whiten(advantages, mask)\n",
    "    \n",
    "    print(f\"\\n最终结果 - 优势形状: {advantages.shape}, 回报形状: {returns.shape}\")\n",
    "    return values, advantages, returns\n",
    "\n",
    "# 测试GAE计算\n",
    "_, advantages, returns = compute_advantages(values, rewards, full_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对数概率计算 - 输入logits形状: torch.Size([1, 10, 12]), 标签形状: torch.Size([1, 10])\n",
      "对数概率分布形状: torch.Size([1, 10, 12])\n",
      "收集对应标签的对数概率形状: torch.Size([1, 10, 1])\n",
      "最终对数概率形状: torch.Size([1, 10])\n",
      "价值模型 - 嵌入输出形状: torch.Size([1, 10, 8])\n",
      "价值模型 - LSTM输出形状: torch.Size([1, 10, 8])\n",
      "价值模型 - 输出价值形状: torch.Size([1, 10])\n",
      "PPO损失 - 旧logprobs: torch.Size([1, 10]), 新logprobs: torch.Size([1, 10])\n",
      "概率比率形状: torch.Size([1, 10]), 均值: 1.0000\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: -1.4305115314527939e-07\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 0.19876843690872192\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 2.4798240661621094\n",
      "掩码均值 - 输入形状: torch.Size([1, 10]), 掩码形状: torch.Size([1, 10]), 输出: 0.0\n"
     ]
    }
   ],
   "source": [
    "def clip_by_value(x, min_val, max_val):\n",
    "    \"\"\"裁剪张量值到指定范围\"\"\"\n",
    "    return torch.max(torch.min(x, max_val), min_val)\n",
    "\n",
    "def entropy_from_logits(logits):\n",
    "    \"\"\"计算策略熵\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=-1)  # [batch, seq]\n",
    "    return entropy\n",
    "\n",
    "def ppo_loss(old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns,\n",
    "           cliprange=0.2, cliprange_value=0.2, vf_coef=0.1):\n",
    "    \"\"\"计算PPO损失函数\"\"\"\n",
    "    print(f\"PPO损失 - 旧logprobs: {old_logprobs.shape}, 新logprobs: {logprobs.shape}\")\n",
    "    \n",
    "    # 1. 计算概率比率 r(θ) = π_θ/π_θ_old\n",
    "    ratio = torch.exp(logprobs - old_logprobs)  # [batch, seq]\n",
    "    print(f\"概率比率形状: {ratio.shape}, 均值: {ratio.mean().item():.4f}\")\n",
    "    \n",
    "    # 2. 计算策略损失\n",
    "    # 原始策略梯度: -advantages * ratio\n",
    "    # 裁剪策略梯度: -advantages * clip(ratio, 1-ε, 1+ε)\n",
    "    pg_losses1 = -advantages * ratio\n",
    "    pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - cliprange, 1.0 + cliprange)\n",
    "    \n",
    "    # 取两者中较大值（较小的收益）\n",
    "    pg_loss = masked_mean(torch.max(pg_losses1, pg_losses2), mask)\n",
    "    \n",
    "    # 3. 计算价值损失\n",
    "    # 裁剪新的价值预测，防止过大更新\n",
    "    vpredclipped = clip_by_value(\n",
    "        vpreds,\n",
    "        values - cliprange_value,\n",
    "        values + cliprange_value,\n",
    "    )\n",
    "    \n",
    "    vf_losses1 = (vpreds - returns) ** 2\n",
    "    vf_losses2 = (vpredclipped - returns) ** 2\n",
    "    vf_loss = 0.5 * masked_mean(torch.max(vf_losses1, vf_losses2), mask)\n",
    "    \n",
    "    # 4. 计算总损失\n",
    "    loss = pg_loss + vf_coef * vf_loss\n",
    "    \n",
    "    # 5. 计算熵和KL散度统计信息\n",
    "    entropy = masked_mean(entropy_from_logits(logits), mask)\n",
    "    approxkl = 0.5 * masked_mean((logprobs - old_logprobs) ** 2, mask)\n",
    "    \n",
    "    return pg_loss, vf_coef * vf_loss, {\n",
    "        \"policy_loss\": pg_loss.item(),\n",
    "        \"value_loss\": vf_loss.item(),\n",
    "        \"total_loss\": loss.item(),\n",
    "        \"entropy\": entropy.item(),\n",
    "        \"approx_kl\": approxkl.item()\n",
    "    }\n",
    "\n",
    "# 生成新的预测值\n",
    "new_policy_outputs = policy_model(full_ids)\n",
    "new_policy_logits = new_policy_outputs.logits\n",
    "new_policy_logprobs = logprobs_from_logits(new_policy_logits, full_ids)\n",
    "new_values = critic_model(full_ids)\n",
    "\n",
    "# 测试PPO损失\n",
    "pg_loss, vf_loss, stats = ppo_loss(\n",
    "    policy_logprobs, values, new_policy_logits, new_values, new_policy_logprobs,\n",
    "    full_mask, advantages, returns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
