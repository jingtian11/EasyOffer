腾讯pcg大模型一面面经📝0324更新
1. 请用3-5分钟时间自我介绍
2. 为什么现在很多研究开始关注 MoE架构？与传统的 Dense 模型相比，MoE 的优势和适用场景是什么？
3. DeepSpeed 微调 Qwen2 - 72B 时，ZeRO - 1、ZeRO - 2、ZeRO - 3 的区别是什么？
4. DeepSpeed ZeRO - 3 模式下，每一张卡占用的显存大概是多少？为什么？
5. 除了 DeepSpeed，微调大模型时还使用过哪些优化方法？优缺点是什么？
6. 大模型训练和推理的完整流程，并说明 SFT和 RLHF在其中的作用。
7. 强化学习算法除了 PPO 和 DPO还有啥，DeepSeek 使用的 GRPO 相比于 GPT 的 PPO 做了哪些改进？
8. 为什么需要在 SFT 之后进行 RLHF？只用 SFT，是否可以达到类似的效果？为什么？
9. 反转链表，最长连续子串